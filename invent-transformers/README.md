# hello kind soul
- this is a narrativized and ascii-ed riff on Gwern's "[You could have invented transformers](https://gwern.net/blog/2025/you-could-have-invented-transformers)"
- it is broken down into the following sections:
    1. The Early Days - From n-grams through embeddings to early neural language models
    2. The Search for Global Context - Convolutions, dilated convolutions, MLP-Mixer, and dynamic weights
    3. When Words Learned to Choose - The birth of attention (Q,K,V mechanism)
    4. The Complete Mind - Multi-head attention, positional embeddings, and the full transformer
- [begin here...](01-the-early-days.md)
